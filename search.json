[
  {
    "objectID": "slides/techlunch31032025.html#our-program",
    "href": "slides/techlunch31032025.html#our-program",
    "title": "Automating online data collection using web-scraping",
    "section": "Our program",
    "text": "Our program\n\nWhy would I want to automate web-scraping?\nThe right tool for the job‚Ä¶\n‚Ä¶ but how do I know the job?\nSome practical examples and code\nQ&A\n(Optional: some funny statistics)"
  },
  {
    "objectID": "slides/techlunch31032025.html#what-i-used-it-for",
    "href": "slides/techlunch31032025.html#what-i-used-it-for",
    "title": "Automating online data collection using web-scraping",
    "section": "What I used it for",
    "text": "What I used it for\n\n\nexporting stock developments\ndetermining median hotel prices\nextracting word frequency\ncircumventing copyright protection (flipbook) ü§´\nmaking a üí©ton of data entries"
  },
  {
    "objectID": "slides/techlunch31032025.html#in-general",
    "href": "slides/techlunch31032025.html#in-general",
    "title": "Automating online data collection using web-scraping",
    "section": "In general",
    "text": "In general\n\nStructured data from webpages without export button\nAnalyzing online-data (forums, product reviews, ‚Ä¶)\nSystematic interactions with specific sites\nFun personal projects üòä\nüí≠‚Ä¶\n\nBut!! Be mindful of the ethical, regulatory and legal impact of your project."
  },
  {
    "objectID": "slides/techlunch31032025.html#the-right-tool-for-the-job",
    "href": "slides/techlunch31032025.html#the-right-tool-for-the-job",
    "title": "Automating online data collection using web-scraping",
    "section": "The right tool for the job‚Ä¶",
    "text": "The right tool for the job‚Ä¶"
  },
  {
    "objectID": "slides/techlunch31032025.html#selenium",
    "href": "slides/techlunch31032025.html#selenium",
    "title": "Automating online data collection using web-scraping",
    "section": "Selenium",
    "text": "Selenium\n\nautomated interaction with a web-browser\n\n\n\nPro\n\ninteraction with web-browser\ninput data (e.g.¬†forms, text)\ninteraction with dynamic sites (JavaScript)\n\n\nCon\n\nslow\nlearning curve\npain in the butt sometimes"
  },
  {
    "objectID": "slides/techlunch31032025.html#beautifulsoup4",
    "href": "slides/techlunch31032025.html#beautifulsoup4",
    "title": "Automating online data collection using web-scraping",
    "section": "BeautifulSoup4",
    "text": "BeautifulSoup4\n\nextracts HTML structures and contents from webpages\n\n\n\nPro\n\nextracting html components of a single URL / a list of URLs where structure is standardized\neasy translations of objects into dataframes\n\n\nCon\n\ninteraction with web-browser\ninput data (e.g.¬†forms, text)\ninteraction with dynamic sites (JavaScript)"
  },
  {
    "objectID": "slides/techlunch31032025.html#beautifulsoup",
    "href": "slides/techlunch31032025.html#beautifulsoup",
    "title": "Automating online data collection using web-scraping",
    "section": "Beautifulsoup",
    "text": "Beautifulsoup\nHow many chairs (and ‚Äúsub-chairs‚Äù) do we have at the psychological Institute in Zurich?\n\n5 chairs and 35 ‚Äúsub-chairs‚Äù\n5 chairs and 20 ‚Äúsub-chairs‚Äù\n5 chairs and 29 ‚Äúsub-chairs‚Äù"
  },
  {
    "objectID": "slides/techlunch31032025.html#beautifulsoup-1",
    "href": "slides/techlunch31032025.html#beautifulsoup-1",
    "title": "Automating online data collection using web-scraping",
    "section": "Beautifulsoup",
    "text": "Beautifulsoup\nHow many chairs (and ‚Äúsub-chairs‚Äù) do we have at the psychological Institute in Zurich?\n\n5 chairs and 35 ‚Äúsub-chairs‚Äù\n5 chairs and 20 ‚Äúsub-chairs‚Äù\n5 chairs and 29 ‚Äúsub-chairs‚Äù"
  },
  {
    "objectID": "slides/techlunch31032025.html#beautifulsoup-2",
    "href": "slides/techlunch31032025.html#beautifulsoup-2",
    "title": "Automating online data collection using web-scraping",
    "section": "Beautifulsoup",
    "text": "Beautifulsoup\nimport requests\nfrom bs4 import BeautifulSoup\nimport pandas as pd\n\n\nbase_url = \"https://www.psychologie.uzh.ch\"\noverview_url = base_url + \"/de/bereiche/uebersicht.html\"\n\nresponse = requests.get(overview_url)\nsoup = BeautifulSoup(response.text,\"html.parser\")\n\n\n\n## Level 1 subpages\nsubpage_links = []\nall_data = []\n\n\nfor link in soup.find_all(\"a\",href=True):\n    href = link[\"href\"]\n    if href.startswith(\"/de/bereiche/\") and href.endswith(\".html\"):\n        subpage_links.append(base_url+href)\n        print(base_url+href)\n\nfor subpage_url in subpage_links:\n    \n    print(f\"\\nScraping {subpage_url}\")\n    response = requests.get(subpage_url)\n    sub_soup = BeautifulSoup(response.text,\"html.parser\")\n\n    chair = sub_soup.find(\"h1\") ## e.g. Entwicklungspsychologie\n    chair_name = chair.get_text(strip=True)\n    print(chair_name)\n\n    tables = sub_soup.find_all(\"table\", class_=\"basic\") ## e.g. \"Entwicklungspsychologie: Kinder- und S√§uglingsalter\"\n\n    for i, table in enumerate(tables):\n        print(f\"\\nTable {i+1} in {subpage_url}\")\n        table_data = []\n        table_data.append(chair_name)\n        rows = table.find_all(\"tr\")\n\n        for i, row in enumerate(rows):\n        \n            cells = row.find_all([\"th\",\"td\"])\n            length = len(cells)\n            print(f\"\\nlen: {length}\")\n\n            if length == 2:\n                title = cells[0].get_text(strip=True) if len(cells) &gt; 0 else \"\"\n                table_data.append(title)\n                link = cells[1].find(\"a\",href=True)\n                table_data.append(link)\n                print(f\"\\niteration: {i}\")\n                print(\"Length = 2\")\n                print(f\"\\nTitle: {title}\")\n                print(f\"\\nLink: {link}\")\n                if link:\n                    full_description = []\n                    subsubpage_link = base_url + link[\"href\"]\n                    subresponse = requests.get(subsubpage_link)\n                    subsub_soup = BeautifulSoup(subresponse.text, \"html.parser\")\n                    description = subsub_soup.find(\"section\", class_=\"ContentArea\")\n                    \n                    if description:\n                        section_text = description.get_text(strip=True, separator=\" \")\n                        full_description.append(section_text)\n                        \n                    else:\n                        print(\"No section found\")\n                    description_text = \" \".join(full_description)\n                    table_data.append(description_text)\n                    print(f\"\\ndescription: {description_text}\")\n            elif length == 1 and i == 1:\n                print(f\"\\niteration: {i}\")\n                print(\"Length = 1\")\n                name = cells[0].get_text(strip=True) if len(cells) &gt; 0 else \"\"\n                print(f\"\\nName: {name}\")\n                table_data.append(name)\n            elif length == 1 and i == 2:\n                print(f\"\\niteration: {i}\")\n                print(\"Length = 1\")\n                position = cells[0].get_text(strip=True) if len(cells) &gt; 0 else \"\"\n                print(f\"\\nPosition: {position}\")\n                table_data.append(position)\n            \n            print(table_data)\n            all_data.append(table_data)\n\ndf = pd.DataFrame(all_data)\ndf.to_csv(\"beautifulsoup.csv\",index=False,sep=\";\",quoting=1,encoding=\"utf-8-sig\")"
  },
  {
    "objectID": "slides/techlunch31032025.html#selenium-1",
    "href": "slides/techlunch31032025.html#selenium-1",
    "title": "Automating online data collection using web-scraping",
    "section": "Selenium",
    "text": "Selenium\nWhat is the most-watched video on the youtube channel of UZH?\n\nDer flexible Schweif des Prions vergiftet die Hirnzellen (en: the flexible tail of the prion poisons brain cells)\nSchwebebahn mit Hochtemperatur-Supraleitung (en: Suspension railroad with high-temperature superconductivity)\nMoosforschung (en: moss research)"
  },
  {
    "objectID": "slides/techlunch31032025.html#selenium-2",
    "href": "slides/techlunch31032025.html#selenium-2",
    "title": "Automating online data collection using web-scraping",
    "section": "Selenium",
    "text": "Selenium\nWhat is the most-watched video on the youtube channel of UZH?\n\nDer flexible Schweif des Prions vergiftet die Hirnzellen (en: the flexible tail of the prion poisons brain cells)\nSchwebebahn mit Hochtemperatur-Supraleitung (en: Suspension railroad with high-temperature superconductivity)\nMoosforschung (en: moss research)"
  },
  {
    "objectID": "slides/techlunch31032025.html#selenium-3",
    "href": "slides/techlunch31032025.html#selenium-3",
    "title": "Automating online data collection using web-scraping",
    "section": "Selenium",
    "text": "Selenium\nfrom selenium import webdriver\nfrom selenium.webdriver.common.by import By\nfrom selenium.webdriver.common.keys import Keys\nfrom selenium.webdriver.firefox.service import Service\nfrom webdriver_manager.firefox import GeckoDriverManager\nfrom selenium.webdriver.firefox.options import Options\nimport time\nimport re\nimport pandas as pd\n\ndata = [] \ndef text_before(text,delimiter):\n    return text.split(delimiter)[0] if delimiter in text else text\ndef text_after(text, delimiter):\n    return text.split(delimiter, 1)[1] if delimiter in text else \"\"\n\nfirefox_options = Options()\n#firefox_options.add_argument(\"--headless\")  # if you don't want to open a window\nfirefox_options.add_argument(\"--disable-gpu\")\n\ndriver = webdriver.Firefox(service=Service(GeckoDriverManager().install()), options=firefox_options)\n\nchannel_url = 'https://www.youtube.com/@uzhch/videos'\ndriver.get(channel_url)\ntime.sleep(5)\n\n## accept all cookies\ntry:\n    accept_button = driver.find_element(By.XPATH, \"/html/body/c-wiz/div/div/div/div[2]/div[1]/div[3]/div[1]/form[2]/div/div/button/span\")\n    accept_button.click()\n    print(\"Cookies accepted.\")\nexcept Exception as e:\n    print(f\"Could not accept cookies: {e}\")\n\ntime.sleep(5)\n\nstart_height = driver.execute_script(\"return document.documentElement.scrollHeight\")\nprint(start_height)\nmax_attempts = 100\nattempt = 0\n\nwhile attempt &lt; max_attempts:    \n    html = driver.find_element(By.TAG_NAME, 'html')\n    html.send_keys(Keys.END)\n    time.sleep(3)\n    new_height = driver.execute_script(\"return document.documentElement.scrollHeight\")\n    print(new_height)\n    if new_height == start_height:\n        break\n    else: \n        start_height = new_height\n\n    attempt+=1\n\nlinks = driver.find_elements(By.CSS_SELECTOR, \"a#video-title-link\")\naria_labels = [link.get_attribute(\"aria-label\") for link in links]\nfor label in aria_labels:\n    print(label)\n\n    title = text_before(label,\"by Universit√§t Z√ºrich\")\n    print(title)\n    \n    views = text_after(label,\"by Universit√§t Z√ºrich\")\n    views = text_before(views,\" views \")\n    print(views)\n    \n    date = text_after(label, views+\" views \")\n    date = text_before(date,\" ago \")\n    print(date)\n    \n    duration = text_after(label,date+\" ago \")\n    print(duration)\n\n    data.append([title, views, date, duration])\n\ndf = pd.DataFrame(data, columns=[\"Title\", \"Views\", \"Date\", \"Duration\"])\ndf.to_csv(\"youtube_data.csv\",index=False,quoting=1,sep=\";\",encoding=\"utf-8\")"
  },
  {
    "objectID": "slides/techlunch31032025.html#qa",
    "href": "slides/techlunch31032025.html#qa",
    "title": "Automating online data collection using web-scraping",
    "section": "Q&A",
    "text": "Q&A"
  },
  {
    "objectID": "slides/techlunch31032025.html#amount-of-videos-uploaded-per-year-on-uzhs-youtube-channel",
    "href": "slides/techlunch31032025.html#amount-of-videos-uploaded-per-year-on-uzhs-youtube-channel",
    "title": "Automating online data collection using web-scraping",
    "section": "Amount of videos uploaded per year on UZH‚Äôs youtube channel",
    "text": "Amount of videos uploaded per year on UZH‚Äôs youtube channel"
  },
  {
    "objectID": "slides/techlunch31032025.html#duration-of-videos-across-time",
    "href": "slides/techlunch31032025.html#duration-of-videos-across-time",
    "title": "Automating online data collection using web-scraping",
    "section": "Duration of Videos across time",
    "text": "Duration of Videos across time"
  },
  {
    "objectID": "slides/techlunch31032025.html#most-frequently-used-words-in-descriptions-of-our-sub-chairs",
    "href": "slides/techlunch31032025.html#most-frequently-used-words-in-descriptions-of-our-sub-chairs",
    "title": "Automating online data collection using web-scraping",
    "section": "Most frequently used words in descriptions of our (sub-)chairs",
    "text": "Most frequently used words in descriptions of our (sub-)chairs"
  },
  {
    "objectID": "slides/techlunch31032025.html#oh-hello-there",
    "href": "slides/techlunch31032025.html#oh-hello-there",
    "title": "Automating online data collection using web-scraping",
    "section": "‚Ä¶ oh hello there!",
    "text": "‚Ä¶ oh hello there!"
  },
  {
    "objectID": "slides/techlunch31032025.html#length-of-descriptions-in-words",
    "href": "slides/techlunch31032025.html#length-of-descriptions-in-words",
    "title": "Automating online data collection using web-scraping",
    "section": "Length of descriptions in words",
    "text": "Length of descriptions in words"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Slides",
    "section": "",
    "text": "Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Title\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\nDate\n\n\nTitle\n\n\nSubtitle\n\n\n\n\n\n\nMar 31, 2025\n\n\nAutomating online data collection using web-scraping\n\n\nTech Lunch @ CBDR\n\n\n\n\n\nNo matching items"
  }
]